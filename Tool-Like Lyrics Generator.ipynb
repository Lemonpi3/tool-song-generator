{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from keras.preprocessing import sequence\n",
    "import keras \n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96238"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_file = \"./data/ToolLyrics.txt\"\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='cp1252')\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186149"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cleaning un necesary spaces\n",
    "text = text.replace(\"\\r\",\"\\n\").replace(\"\\n\\n\\n\\n\",\"\").split(\"\\n\")\n",
    "import re\n",
    "\n",
    "cleaned_text = \"\\n\"\n",
    "for line in text:\n",
    "    if line.startswith(\"[\") and line.endswith(\"]\"):\n",
    "        line = \"\"\n",
    "        \n",
    "text = cleaned_text.join(text)\n",
    "text = text + \"\\n\" + text #I experimented by doubleing the training data in this way and the models archived better results faster.\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))\n",
    "\n",
    "char2idx = {u:i for i,u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "def text_to_int(text):\n",
    "    return np.array([char2idx[c] for c in text])\n",
    "\n",
    "text_as_int = text_to_int(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Text: {text[:200]}\\nEncoded: {text_as_int[:200]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_text(ints):\n",
    "    try:\n",
    "        ints = ints.numpy()\n",
    "    except:\n",
    "        print('Already a numpy array')\n",
    "    return ''.join(idx2char[ints])\n",
    "\n",
    "# print(int_to_text(text_as_int[:200]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_lenght = 100\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "sequences = char_dataset.batch(seq_lenght+1,drop_remainder = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    \n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE = len(vocab)\n",
    "EMBEDING_DIM = 256\n",
    "RNN_UNITS = 512\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_30 (Embedding)    (64, None, 256)           23296     \n",
      "                                                                 \n",
      " lstm_33 (LSTM)              (64, None, 512)           1574912   \n",
      "                                                                 \n",
      " lstm_34 (LSTM)              (64, None, 512)           2099200   \n",
      "                                                                 \n",
      " dense_30 (Dense)            (64, None, 91)            46683     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,744,091\n",
      "Trainable params: 3,744,091\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(vocab_size, embeding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embeding_dim, batch_input_shape=[batch_size,None]),\n",
    "        tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model= build_model(VOCAB_SIZE,EMBEDING_DIM,RNN_UNITS,BATCH_SIZE)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels,logits,from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True,\n",
    "    save_freq=1120,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded: ./checkpoints/lyricsgencheckpoints\\ckpt_40\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = './checkpoints/lyricsgencheckpoints'\n",
    "\n",
    "try:\n",
    "    model= build_model(VOCAB_SIZE,EMBEDING_DIM,RNN_UNITS,64)\n",
    "\n",
    "    model.load_weights(tf.train.latest_checkpoint(checkpoint_dir)).expect_partial()\n",
    "\n",
    "#     model.build(tf.TensorShape([1,None]))\n",
    "    print(f'\\nLoaded: {tf.train.latest_checkpoint(checkpoint_dir)}')\n",
    "\n",
    "except:\n",
    "    print(\"\\nNo checkpoint to load.\")\n",
    "\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.000001), loss=loss)\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "28/28 [==============================] - 12s 349ms/step - loss: 0.1451\n",
      "Epoch 2/40\n",
      "28/28 [==============================] - 10s 348ms/step - loss: 0.1476\n",
      "Epoch 3/40\n",
      "28/28 [==============================] - 7s 247ms/step - loss: 0.1471\n",
      "Epoch 4/40\n",
      "28/28 [==============================] - 7s 251ms/step - loss: 0.1457\n",
      "Epoch 5/40\n",
      "28/28 [==============================] - 10s 365ms/step - loss: 0.1451\n",
      "Epoch 6/40\n",
      "28/28 [==============================] - 10s 354ms/step - loss: 0.1465\n",
      "Epoch 7/40\n",
      "28/28 [==============================] - 10s 350ms/step - loss: 0.1449\n",
      "Epoch 8/40\n",
      "28/28 [==============================] - 10s 349ms/step - loss: 0.1449\n",
      "Epoch 9/40\n",
      "28/28 [==============================] - 10s 349ms/step - loss: 0.1432\n",
      "Epoch 10/40\n",
      "28/28 [==============================] - 10s 349ms/step - loss: 0.1415\n",
      "Epoch 11/40\n",
      "28/28 [==============================] - 10s 349ms/step - loss: 0.1434\n",
      "Epoch 12/40\n",
      "28/28 [==============================] - 10s 353ms/step - loss: 0.1415\n",
      "Epoch 13/40\n",
      "28/28 [==============================] - 10s 351ms/step - loss: 0.1406\n",
      "Epoch 14/40\n",
      "28/28 [==============================] - 10s 349ms/step - loss: 0.1422\n",
      "Epoch 15/40\n",
      "28/28 [==============================] - 10s 350ms/step - loss: 0.1408\n",
      "Epoch 16/40\n",
      "28/28 [==============================] - 10s 352ms/step - loss: 0.1388\n",
      "Epoch 17/40\n",
      "28/28 [==============================] - 10s 356ms/step - loss: 0.1384\n",
      "Epoch 18/40\n",
      "28/28 [==============================] - 10s 352ms/step - loss: 0.1383\n",
      "Epoch 19/40\n",
      "28/28 [==============================] - 10s 351ms/step - loss: 0.1379\n",
      "Epoch 20/40\n",
      "28/28 [==============================] - 10s 357ms/step - loss: 0.1368\n",
      "Epoch 21/40\n",
      "28/28 [==============================] - 10s 354ms/step - loss: 0.1374\n",
      "Epoch 22/40\n",
      "28/28 [==============================] - 10s 351ms/step - loss: 0.1376\n",
      "Epoch 23/40\n",
      "28/28 [==============================] - 10s 351ms/step - loss: 0.1389\n",
      "Epoch 24/40\n",
      "28/28 [==============================] - 10s 351ms/step - loss: 0.1363\n",
      "Epoch 25/40\n",
      "28/28 [==============================] - 10s 352ms/step - loss: 0.1369\n",
      "Epoch 26/40\n",
      "28/28 [==============================] - 10s 356ms/step - loss: 0.1361\n",
      "Epoch 27/40\n",
      "28/28 [==============================] - 10s 352ms/step - loss: 0.1330\n",
      "Epoch 28/40\n",
      "28/28 [==============================] - 10s 351ms/step - loss: 0.1348\n",
      "Epoch 29/40\n",
      "28/28 [==============================] - 10s 351ms/step - loss: 0.1325\n",
      "Epoch 30/40\n",
      "28/28 [==============================] - 10s 353ms/step - loss: 0.1351\n",
      "Epoch 31/40\n",
      "28/28 [==============================] - 10s 356ms/step - loss: 0.1326\n",
      "Epoch 32/40\n",
      "28/28 [==============================] - 10s 359ms/step - loss: 0.1320\n",
      "Epoch 33/40\n",
      "28/28 [==============================] - 10s 357ms/step - loss: 0.1311\n",
      "Epoch 34/40\n",
      "28/28 [==============================] - 10s 356ms/step - loss: 0.1302\n",
      "Epoch 35/40\n",
      "28/28 [==============================] - 9s 302ms/step - loss: 0.1299\n",
      "Epoch 36/40\n",
      "28/28 [==============================] - 8s 297ms/step - loss: 0.1296\n",
      "Epoch 37/40\n",
      "28/28 [==============================] - 10s 350ms/step - loss: 0.1319\n",
      "Epoch 38/40\n",
      "28/28 [==============================] - 10s 349ms/step - loss: 0.1307\n",
      "Epoch 39/40\n",
      "28/28 [==============================] - 9s 326ms/step - loss: 0.1295\n",
      "Epoch 40/40\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.1288\n",
      "Epoch 00040: saving model to ./checkpoints/lyricsgencheckpoints\\ckpt_40\n",
      "28/28 [==============================] - 4s 152ms/step - loss: 0.1293\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(data, epochs=40, callbacks = [checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./checkpoints/lyricsgencheckpoints\\ckpt_40\n"
     ]
    }
   ],
   "source": [
    "model= build_model(VOCAB_SIZE,EMBEDING_DIM,RNN_UNITS,1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir)).expect_partial()\n",
    "\n",
    "model.build(tf.TensorShape([1,None]))\n",
    "\n",
    "print(f'{tf.train.latest_checkpoint(checkpoint_dir)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, num_generate=400, temperature=1):\n",
    "    \n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval,0)\n",
    "    \n",
    "    text_generated = []\n",
    "    \n",
    "    model.reset_states()\n",
    "    \n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions,0)\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions,num_samples=1)[-1,0].numpy()\n",
    "        input_eval = tf.expand_dims([predicted_id] , 0)\n",
    "        \n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "        \n",
    "    return start_string + ''.join(text_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_words = input(\"Input Starting words\")\n",
    "lyrics_lenght = 1000\n",
    "temperature = 1\n",
    "def input_lenght():\n",
    "    try:\n",
    "        lyrics_lenght = int(input(\"Input song lenght, default = 1000\"))\n",
    "    except:\n",
    "        print(\"please input a number\")\n",
    "        input_lenght()\n",
    "\n",
    "def input_temp():\n",
    "    try:\n",
    "        lyrics_lenght = float(input(\"Input temperature, default = 1.0\"))\n",
    "    except:\n",
    "        print(\"please input a number\")\n",
    "        input_lenght()\n",
    "        \n",
    "input_lenght()\n",
    "input_temp()\n",
    "\n",
    "print(f\"generateing song with: {starting_words}, {lyrics_lenght}, {temperature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyric = generate_text(model,starting_words,lyrics_lenght,temperature)\n",
    "print(lyric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE_NAME = \"test\"\n",
    "\n",
    "with open(f\"./outputs/lyrics-generator-outputs/{OUTPUT_FILE_NAME}.txt\", \"w\") as text_file:\n",
    "    print(f\"{lyric}\", file=text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
